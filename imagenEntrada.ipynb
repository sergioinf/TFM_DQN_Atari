{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with: cuda\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Breakout-v5')\n",
    "#env = gym.make('ALE/SpaceInvaders-v5')\n",
    "\n",
    "\n",
    "\n",
    "numAcciones = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training with:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image():\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                    #T.Resize(95, interpolation=Image.CUBIC),\n",
    "                    T.Grayscale(num_output_channels=1),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "    screen = resize(screen).unsqueeze(0)\n",
    "    return screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def save(self, state, action, next_state, reward):\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices     = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        \n",
    "        states      = [self.memory[idx][0] for idx in indices]\n",
    "        actions     = [self.memory[idx][1] for idx in indices]\n",
    "        next_states = [self.memory[idx][2] for idx in indices]\n",
    "        rewards     = [self.memory[idx][3] for idx in indices]\n",
    "        \n",
    "        return states, actions, rewards, next_states\n",
    "        #return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(10000)\n",
    "BATCH_SIZE = 128\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = .999985\n",
    "EPS_MIN = 0.02\n",
    "epsilon = EPS_START\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    estado, accion, recompensa, estado_sig = batch\n",
    "\n",
    "\n",
    "    print(estado_sig[0])\n",
    "    try:\n",
    "        listaNones = estado_sig.index(None)\n",
    "    except:\n",
    "        listaNones = []\n",
    "\n",
    "    tensor_estados      = torch.tensor(estado).to(device)\n",
    "    tensor_accion       = torch.tensor(accion).to(device)\n",
    "    tensor_recompensa   = torch.tensor(recompensa).to(device)\n",
    "    tensor_estado_sig   = torch.tensor(estado_sig).to(device)\n",
    "\n",
    "    tensor_estado_sig[listaNones] = tensor_estados[listaNones]\n",
    "\n",
    "    Qvalues = red_politica(tensor_estados).max(1)[0].view(1, 1)\n",
    "    print(Qvalues)\n",
    "    Qvalues = red_politica(tensor_estados).gather(1, tensor_accion.unsqueeze(-1)).squeeze(-1)\n",
    "    print(Qvalues)\n",
    "\n",
    "    \n",
    "    QpValues = red_objetivo(tensor_estado_sig).max(1)[0].view(1, 1)\n",
    "    QpValues[listaNones] = 0.0\n",
    "    \n",
    "\n",
    "    valorEsperado = QpValues * gamma + tensor_recompensa\n",
    "\n",
    "        \n",
    "    loss = nn.MSELoss() (Qvalues, valorEsperado)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        \"\"\"Para calcular correctamente la salida, tenemos que linealizarla, esto depende de las dimensiones\n",
    "        de las imagenes de entrada y de los parÃ¡metros introducidos\"\"\"\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    \"\"\"Devuelve un vector con el valor de las acciones posibles\"\"\"\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = get_image()\n",
    "_, _, screen_height, screen_width = screen.shape\n",
    "\n",
    "red_politica = DQN(screen_height, screen_width, numAcciones).to(device)\n",
    "red_objetivo = DQN(screen_height, screen_width, numAcciones).to(device)\n",
    "red_objetivo.load_state_dict(red_politica.state_dict())\n",
    "red_objetivo.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(red_politica.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_selection(state):\n",
    "    global epsilon\n",
    "    e = epsilon\n",
    "    \n",
    "    if epsilon-EPS_DECAY > EPS_MIN:\n",
    "        epsilon = epsilon-EPS_DECAY\n",
    "    else:\n",
    "        epsilon = epsilon\n",
    "        \n",
    "    if random.randint(0, 100)/100 < e:\n",
    "        return torch.tensor([[random.randrange(numAcciones)]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return  red_politica(state).max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Sergio\\TFM\\TFM_DQN_Atari\\imagenEntrada.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000009?line=22'>23</a>\u001b[0m memory\u001b[39m.\u001b[39msave(estado, accion, sig_estado, recompensa)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000009?line=24'>25</a>\u001b[0m estado \u001b[39m=\u001b[39m sig_estado\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000009?line=26'>27</a>\u001b[0m optimize_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000009?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000009?line=29'>30</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32md:\\Sergio\\TFM\\TFM_DQN_Atari\\imagenEntrada.ipynb Cell 6'\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000005?line=12'>13</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000005?line=13'>14</a>\u001b[0m     listaNones \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000005?line=15'>16</a>\u001b[0m tensor_estados      \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(estado)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000005?line=16'>17</a>\u001b[0m tensor_accion       \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(accion)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000005?line=17'>18</a>\u001b[0m tensor_recompensa   \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(recompensa)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "episodios = 1\n",
    "for i in range(episodios):\n",
    "    env.reset()\n",
    "    screen_1 = get_image()\n",
    "    screen_2 = get_image()\n",
    "\n",
    "    estado = screen_2-screen_1\n",
    "\n",
    "    for j in count():\n",
    "        accion = action_selection(estado)\n",
    "        _, recompensa, done, _ = env.step(accion.item())\n",
    "        recompensa = torch.tensor([recompensa], device=device)\n",
    "\n",
    "        screen1 = screen_2\n",
    "        screen_2 = get_image()\n",
    "        \n",
    "        if not done:\n",
    "            sig_estado = screen_2-screen_1\n",
    "        else:\n",
    "            sig_estado = None\n",
    "            break\n",
    "            \n",
    "        memory.save(estado, accion, sig_estado, recompensa)\n",
    "\n",
    "        estado = sig_estado\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        if j == BATCH_SIZE:\n",
    "            break\n",
    "\n",
    "    print(\"Partida {} acabada\".format(i))\n",
    "if i % 10 == 0:\n",
    "    red_objetivo.load_state_dict(red_politica.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(red_objetivo.state_dict(), \"RedObjetivo.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a381338f764c2e2cde69e871487c18b603a6ed2174a69a63573e476f4f348b8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

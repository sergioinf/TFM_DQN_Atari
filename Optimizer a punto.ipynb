{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with: cuda\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Breakout-v5')\n",
    "#env = gym.make('ALE/SpaceInvaders-v5')\n",
    "\n",
    "\n",
    "\n",
    "numAcciones = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training with:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image():\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                    #T.Resize(95, interpolation=Image.CUBIC),\n",
    "                    T.Grayscale(num_output_channels=1),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "    screen = resize(screen).unsqueeze(0)\n",
    "    return screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def save(self, state, action, next_state, reward):\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices     = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        \n",
    "        states      = []\n",
    "        actions     = []\n",
    "        next_states = []\n",
    "        rewards      = []\n",
    "\n",
    "\n",
    "        for idx in indices: \n",
    "            states.append(self.memory[idx][0])\n",
    "            actions.append(self.memory[idx][1])\n",
    "            next_states.append(self.memory[idx][2])\n",
    "            rewards.append(self.memory[idx][3])\n",
    "        \n",
    "        return states, actions, rewards, next_states\n",
    "        #return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(10000)\n",
    "BATCH_SIZE = 128\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = .999985\n",
    "EPS_MIN = 0.02\n",
    "epsilon = EPS_START\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    estado, accion, recompensa, estado_sig = batch\n",
    "\n",
    "    estado = np.array(estado)\n",
    "    estado_sig = np.array(estado_sig)\n",
    "\n",
    "    try:\n",
    "        listaNones = np.where(estado_sig == None)\n",
    "    except:\n",
    "        listaNones = []\n",
    "\n",
    "    #tensor_estados     = torch.Tensor(estado).to('cuda')\n",
    "    tensor_accion       = torch.Tensor(accion).to(device)\n",
    "    tensor_recompensa   = torch.Tensor(recompensa).to(device)\n",
    "    #tensor_estado_sig  = torch.Tensor(estado_sig).to(device)\n",
    "\n",
    "    estado_sig[listaNones] = estado[listaNones]\n",
    "\n",
    "    Qvalues = [red_politica(e).max(1)[0].item() for e in estado]\n",
    "    \n",
    "    #Qvalues = [red_politica(e).gather(1, tensor_accion.unsqueeze(-1)).squeeze(-1) for e in estado]\n",
    "    #print(Qvalues)\n",
    "\n",
    "    \n",
    "    QpValues = [red_objetivo(e).max(1)[0].item() for e in estado_sig]\n",
    "    QpValues = np.array(QpValues)\n",
    "    QpValues[listaNones] = 0.0\n",
    "\n",
    "\n",
    "    # Qvalues = torch.Tensor(Qvalues).to(device)\n",
    "    # QpValues = torch.Tensor(QpValues).to(device)\n",
    "    torch.Tensor(loquesea, requires_grad = True)\n",
    "\n",
    "    valorEsperado = QpValues * gamma + tensor_recompensa\n",
    "\n",
    "    Qvalues.retain_grad()\n",
    "    valorEsperado.retain_grad()\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(Qvalues, valorEsperado.unsqueeze(1))\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    for param in red_politica.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "    # loss = nn.MSELoss() \n",
    "    # output = loss(Qvalues, valorEsperado)\n",
    "    # optimizer.zero_grad()\n",
    "    # output.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        \"\"\"Para calcular correctamente la salida, tenemos que linealizarla, esto depende de las dimensiones\n",
    "        de las imagenes de entrada y de los parÃ¡metros introducidos\"\"\"\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    \"\"\"Devuelve un vector con el valor de las acciones posibles\"\"\"\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = get_image()\n",
    "_, _, screen_height, screen_width = screen.shape\n",
    "\n",
    "red_politica = DQN(screen_height, screen_width, numAcciones).to(device)\n",
    "red_objetivo = DQN(screen_height, screen_width, numAcciones).to(device)\n",
    "red_objetivo.load_state_dict(red_politica.state_dict())\n",
    "red_objetivo.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(red_politica.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_selection(state):\n",
    "    global epsilon\n",
    "    e = epsilon\n",
    "    \n",
    "    if epsilon-EPS_DECAY > EPS_MIN:\n",
    "        epsilon = epsilon-EPS_DECAY\n",
    "    else:\n",
    "        epsilon = epsilon\n",
    "        \n",
    "    if random.randint(0, 100)/100 < e:\n",
    "        return random.randrange(numAcciones)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return  red_politica(state).max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergio\\AppData\\Local\\Temp\\ipykernel_1332\\936108203.py:9: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  estado = np.array(estado)\n",
      "C:\\Users\\Sergio\\AppData\\Local\\Temp\\ipykernel_1332\\936108203.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  estado = np.array(estado)\n",
      "C:\\Users\\Sergio\\AppData\\Local\\Temp\\ipykernel_1332\\936108203.py:10: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  estado_sig = np.array(estado_sig)\n",
      "C:\\Users\\Sergio\\AppData\\Local\\Temp\\ipykernel_1332\\936108203.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  estado_sig = np.array(estado_sig)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "can't retain_grad on Tensor that has requires_grad=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Sergio\\TFM\\TFM_DQN_Atari\\imagenEntrada.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000009?line=22'>23</a>\u001b[0m memory\u001b[39m.\u001b[39msave(estado, accion, sig_estado, recompensa)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000009?line=24'>25</a>\u001b[0m estado \u001b[39m=\u001b[39m sig_estado\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000009?line=26'>27</a>\u001b[0m optimize_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000009?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000009?line=29'>30</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32md:\\Sergio\\TFM\\TFM_DQN_Atari\\imagenEntrada.ipynb Cell 6'\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000005?line=35'>36</a>\u001b[0m QpValues \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(QpValues)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000005?line=37'>38</a>\u001b[0m valorEsperado \u001b[39m=\u001b[39m QpValues \u001b[39m*\u001b[39m gamma \u001b[39m+\u001b[39m tensor_recompensa\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000005?line=39'>40</a>\u001b[0m Qvalues\u001b[39m.\u001b[39;49mretain_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000005?line=40'>41</a>\u001b[0m valorEsperado\u001b[39m.\u001b[39mretain_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000005?line=41'>42</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSmoothL1Loss()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: can't retain_grad on Tensor that has requires_grad=False"
     ]
    }
   ],
   "source": [
    "episodios = 1\n",
    "for i in range(episodios):\n",
    "    env.reset()\n",
    "    screen_1 = get_image()\n",
    "    screen_2 = get_image()\n",
    "\n",
    "    estado = screen_2-screen_1\n",
    "\n",
    "    for j in count():\n",
    "        accion = action_selection(estado)\n",
    "        estadoPrueba, recompensa, done, _ = env.step(accion)\n",
    "        #recompensa = torch.tensor([recompensa], device=device)\n",
    "        \n",
    "        screen1 = screen_2\n",
    "        screen_2 = get_image()\n",
    "        \n",
    "        if not done:\n",
    "            sig_estado = screen_2-screen_1\n",
    "        else:\n",
    "            sig_estado = None\n",
    "            break\n",
    "            \n",
    "        memory.save(estado, accion, sig_estado, recompensa)\n",
    "\n",
    "        estado = sig_estado\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        if j == BATCH_SIZE:\n",
    "            break\n",
    "\n",
    "    print(\"Partida {} acabada\".format(i))\n",
    "if i % 10 == 0:\n",
    "    red_objetivo.load_state_dict(red_politica.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(red_objetivo.state_dict(), \"RedObjetivo.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Sergio\\TFM\\TFM_DQN_Atari\\imagenEntrada.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000013?line=2'>3</a>\u001b[0m listaNones \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere( a \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000013?line=3'>4</a>\u001b[0m a \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m54\u001b[39m,\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000013?line=4'>5</a>\u001b[0m a[listaNones] \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/imagenEntrada.ipynb#ch0000013?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(a)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "a = np.array([None,2,3,54,None, None])\n",
    "\n",
    "listaNones = np.where( a == None)\n",
    "a = [None,2,3,54,None, None]\n",
    "a[listaNones] = 2\n",
    "\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a381338f764c2e2cde69e871487c18b603a6ed2174a69a63573e476f4f348b8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import logging\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pickle as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with: cuda\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Breakout-v5')\n",
    "\n",
    "numAcciones = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training with:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "#------------------- Get image from enviromet ------------------------\n",
    "######################################################################\n",
    "\n",
    "def get_image():\n",
    "    screen = env.render(mode='rgb_array')#.transpose((2, 0, 1))\n",
    "    grayimg = cv2.cvtColor(screen, cv2.COLOR_RGB2GRAY)\n",
    "    scaled = cv2.resize(grayimg, (110, 84))\n",
    "    cropped_image = scaled[0:84, 0:84]\n",
    "\n",
    "    return cropped_image  #  required for some algorithms\n",
    "\n",
    "\n",
    "    # screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # screen = torch.from_numpy(screen)\n",
    "\n",
    "    # resize = T.Compose([T.ToPILImage(),\n",
    "    #                 #T.Resize(95, interpolation=Image.CUBIC),\n",
    "    #                 T.Grayscale(num_output_channels=1),\n",
    "    #                 T.ToTensor()])\n",
    "\n",
    "    # screen = resize(screen).unsqueeze(0)\n",
    "    # return screen\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#--------------------------------------- Memory ---------------------------------------\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def save(self, state, action, next_state, reward):\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices     = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        \n",
    "        states      = []\n",
    "        actions     = []\n",
    "        next_states = []\n",
    "        rewards     = []\n",
    "\n",
    "\n",
    "        for idx in indices: \n",
    "            states.append(self.memory[idx][0])\n",
    "            actions.append(self.memory[idx][1])\n",
    "            next_states.append(self.memory[idx][2])\n",
    "            rewards.append(self.memory[idx][3])\n",
    "        \n",
    "        return states, actions, rewards, next_states\n",
    "        #return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(1000000)\n",
    "BUFFER_CAP = 4\n",
    "BATCH_SIZE = 32\n",
    "EPS_START = 0.9\n",
    "# EPS_DECAY = 0.045\n",
    "# EPS_MIN = 0.02\n",
    "epsilon = EPS_START\n",
    "gamma = 0.99\n",
    "# it = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#------------------------------- Memory Optimizer -------------------------------------\n",
    "#######################################################################################\n",
    "\n",
    "def optimize_model():\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    estado, accion, recompensa, estado_sig = batch\n",
    "\n",
    "    estado = np.array(estado)\n",
    "    estado_sig = np.array(estado_sig)\n",
    "\n",
    "    try:\n",
    "        listaNones = np.where(estado_sig == None)\n",
    "    except:\n",
    "        listaNones = []\n",
    "\n",
    "    tensor_accion       = torch.Tensor(accion).to(device)\n",
    "    tensor_recompensa   = torch.Tensor(recompensa).to(device)\n",
    "\n",
    "    estado_sig[listaNones] = estado[listaNones]\n",
    "\n",
    "    Qvalues = [red_politica(e).max(1)[0].item() for e in estado]\n",
    "    \n",
    "    QpValues = [red_objetivo(e).max(1)[0].item() for e in estado_sig]\n",
    "    QpValues = np.array(QpValues)\n",
    "    QpValues[listaNones] = 0.0\n",
    "\n",
    "\n",
    "    Qvalues = torch.Tensor(Qvalues).to(device)\n",
    "    Qvalues.requires_grad_()\n",
    "    QpValues = torch.Tensor(QpValues).to(device)\n",
    "    QpValues.requires_grad_()\n",
    "\n",
    "    valorEsperado = QpValues * gamma + tensor_recompensa\n",
    "\n",
    "    Qvalues.retain_grad()\n",
    "    valorEsperado.retain_grad()\n",
    "    \n",
    "    \n",
    "\n",
    "    loss = nn.MSELoss() \n",
    "    output = loss(valorEsperado, Qvalues)\n",
    "    optimizer.zero_grad()\n",
    "    output.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "#----------------------------------------- Estructura de la red ------------------------------------------------\n",
    "################################################################################################################\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)\n",
    "        \n",
    "        \"\"\"Para calcular correctamente la salida, tenemos que linealizarla, esto depende de las dimensiones\n",
    "        de las imagenes de entrada y de los parámetros introducidos\"\"\"\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "\n",
    "        self.hl = nn.Linear(linear_input_size, 256)\n",
    "        self.ol = nn.Linear(256, outputs)\n",
    "\n",
    "    \"\"\"Devuelve un vector con el valor de las acciones posibles\"\"\"\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return self.ol(self.hl(x.view(x.size(0), -1)))\n",
    "\n",
    "#####################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redes inicializadas:\n",
      "Redes cargadas:\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "#-------------------- Inicialización de redes ----------------------------\n",
    "##########################################################################\n",
    "screen = get_image()\n",
    "screen_height, screen_width = screen.shape\n",
    "\n",
    "red_politica = DQN(screen_height, screen_width, numAcciones).to(device)\n",
    "red_objetivo = DQN(screen_height, screen_width, numAcciones).to(device)\n",
    "red_objetivo.load_state_dict(red_politica.state_dict())\n",
    "red_objetivo.eval()\n",
    "scoreList = []\n",
    "optimizer = optim.RMSprop(red_politica.parameters())\n",
    "print(\"Redes inicializadas:\")\n",
    "\n",
    "# #----------------------- Carga red de fichero ------------------#\n",
    "# pickle_in = open('listaScore','rb')                       #\n",
    "# scoreList = p.load(pickle_in)                                   #\n",
    "# pickle_in.close()                                               #\n",
    "#                                                                 #\n",
    "\n",
    "# # pickle_in = open('Memory','rb')                           #\n",
    "# # memory.memory = p.load(pickle_in)                               #\n",
    "# # pickle_in.close()                                               #\n",
    "#                                                                 #\n",
    "#                                                                 #\n",
    "# red_politica.load_state_dict(torch.load('RedPolitica.pt'))#\n",
    "# red_objetivo.load_state_dict(torch.load('RedObjetivo.pt'))#\n",
    "# #---------------------------------------------------------------#\n",
    "print(\"Redes cargadas:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#-------------------------------------- Selector de acciones -------------------------------\n",
    "############################################################################################\n",
    "def action_selection(state):\n",
    "    global epsilon\n",
    "    \n",
    "    # epsilon = epsilon-EPS_DECAY/it\n",
    "    # if epsilon < EPS_MIN:\n",
    "    #     epsilon = EPS_MIN\n",
    "    \n",
    "    # it += 1\n",
    "\n",
    "    if random.randint(0, 100)/100 < epsilon:\n",
    "        return random.randrange(numAcciones)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return  red_politica(state).max(1)[1]\n",
    "\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comienzo del entrenamiento:\n",
      "<class 'collections.deque'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 84 at dim 1 (got 210)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Sergio\\TFM\\TFM_DQN_Atari\\DQN.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN.ipynb#ch0000009?line=25'>26</a>\u001b[0m     ImageBuffer\u001b[39m.\u001b[39mappend(estadoSig)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN.ipynb#ch0000009?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(ImageBuffer))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN.ipynb#ch0000009?line=27'>28</a>\u001b[0m     estadoSig \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mTensor(ImageBuffer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN.ipynb#ch0000009?line=28'>29</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN.ipynb#ch0000009?line=29'>30</a>\u001b[0m     estadoSig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 84 at dim 1 (got 210)"
     ]
    }
   ],
   "source": [
    "############################################################################################\n",
    "#-------------------------------------- Bucle de entrenamiento -------------------------------\n",
    "############################################################################################\n",
    "print(\"Comienzo del entrenamiento:\")\n",
    "\n",
    "\n",
    "antesEmpezar = get_image()\n",
    "\n",
    "ImageBufferInicio = deque([antesEmpezar for i in range(BUFFER_CAP)], maxlen=BUFFER_CAP)\n",
    "\n",
    "\n",
    "episodios = 10000000\n",
    "for i in range(episodios):\n",
    "    env.reset()\n",
    "\n",
    "    ImageBuffer = ImageBufferInicio\n",
    "    # Cambiar la diferencia de las pantallas por un cubo con 4 pantallas juntas\n",
    "    estado = torch.Tensor(ImageBuffer)\n",
    "\n",
    "    score = 0\n",
    "    for j in count():\n",
    "        accion = action_selection(estado)\n",
    "        estadoSig, recompensa, done, _ = env.step(accion)\n",
    "        \n",
    "        if not done:\n",
    "            ImageBuffer.append(estadoSig)\n",
    "            print(type(ImageBuffer))\n",
    "            estadoSig = torch.Tensor(ImageBuffer)\n",
    "        else:\n",
    "            estadoSig = None\n",
    "            \n",
    "        memory.save(estado, accion, estadoSig, recompensa)\n",
    "\n",
    "        estado = estadoSig\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            scoreList.append(score)\n",
    "            break\n",
    "    print(\"Partida {} acabada, recompensa acumulada {}\".format(i, score))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        red_objetivo.load_state_dict(red_politica.state_dict())\n",
    "        torch.save(red_objetivo.state_dict(), \"RedObjetivo.pt\")\n",
    "        torch.save(red_politica.state_dict(), \"RedPolitica.pt\")\n",
    "\n",
    "        outputFile = open('Memory', 'wb')\n",
    "        p.dump(memory.memory, outputFile)\n",
    "        outputFile.close()\n",
    "\n",
    "        outputFile = open('listaScore', 'wb')\n",
    "        p.dump(scoreList, outputFile)\n",
    "        outputFile.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a381338f764c2e2cde69e871487c18b603a6ed2174a69a63573e476f4f348b8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

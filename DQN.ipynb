{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Sergio\\TFM\\TFM_DQN_Atari\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import logging\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pickle as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with: cuda\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Breakout-v5')\n",
    "\n",
    "numAcciones = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training with:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#--------------------------- Logger -------------------------\n",
    "#############################################################\n",
    "\n",
    "logging.basicConfig(filename=\"logFile.log\",\n",
    "                    format='%(asctime)s %(message)s',\n",
    "                    filemode='w')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "#------------------- Process the next frame   ------------------------\n",
    "######################################################################\n",
    "\n",
    "def process_image(screen = None):\n",
    "    if screen is None:\n",
    "        screen = env.render(mode='rgb_array')\n",
    "    grayimg = cv2.cvtColor(screen, cv2.COLOR_RGB2GRAY)\n",
    "    scaled = cv2.resize(grayimg, (84, 110))\n",
    "    cropped_image = scaled[26:110, 0:84]\n",
    "\n",
    "    return cropped_image\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#--------------------------------------- Memory ---------------------------------------\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def save(self, state, action, next_state, reward):\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices     = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        \n",
    "        states      = []\n",
    "        actions     = []\n",
    "        next_states = []\n",
    "        rewards     = []\n",
    "\n",
    "\n",
    "        for idx in indices: \n",
    "            states.append(self.memory[idx][0])\n",
    "            actions.append(self.memory[idx][1])\n",
    "            next_states.append(self.memory[idx][2])\n",
    "            rewards.append(self.memory[idx][3])\n",
    "        \n",
    "        return states, actions, rewards, next_states\n",
    "        #return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(1000000)\n",
    "BUFFER_CAP = 4\n",
    "BATCH_SIZE = 32\n",
    "EPS_START = 1\n",
    "EPS_DECAY = 0.0000001\n",
    "EPS_MIN = 0.1\n",
    "epsilon = EPS_START\n",
    "gamma = 0.99\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#------------------------------- Memory Optimizer -------------------------------------\n",
    "#######################################################################################\n",
    "\n",
    "def optimize_model():\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    estado, accion, recompensa, estado_sig = batch\n",
    "\n",
    "    estado = np.array(estado)\n",
    "    estado_sig = np.array(estado_sig)\n",
    "\n",
    "    try:\n",
    "        listaNones = np.where(estado_sig == None)\n",
    "    except:\n",
    "        listaNones = []\n",
    "\n",
    "    tensor_accion       = torch.Tensor(accion).to(device)\n",
    "    tensor_recompensa   = torch.Tensor(recompensa).to(device)\n",
    "\n",
    "    estado_sig[listaNones] = estado[listaNones]\n",
    "\n",
    "    Qvalues = [red_politica(e).max(1)[0].item() for e in estado]\n",
    "    \n",
    "    QpValues = [red_objetivo(e).max(1)[0].item() for e in estado_sig]\n",
    "    QpValues = np.array(QpValues)\n",
    "    QpValues[listaNones] = 0.0\n",
    "\n",
    "\n",
    "    Qvalues = torch.Tensor(Qvalues).to(device)\n",
    "    Qvalues.requires_grad_()\n",
    "    QpValues = torch.Tensor(QpValues).to(device)\n",
    "    QpValues.requires_grad_()\n",
    "\n",
    "    valorEsperado = QpValues * gamma + tensor_recompensa\n",
    "    \n",
    "\n",
    "    Qvalues.retain_grad()\n",
    "    valorEsperado.retain_grad()\n",
    "    \n",
    "    loss = nn.MSELoss() \n",
    "    output = loss(Qvalues, valorEsperado)\n",
    "\n",
    "    medida = np.mean([i.item() for i in valorEsperado])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return medida\n",
    "\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "#----------------------------------------- Estructura de la red ------------------------------------------------\n",
    "################################################################################################################\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)\n",
    "        \n",
    "        \"\"\"Para calcular correctamente la salida, tenemos que linealizarla, esto depende de las dimensiones\n",
    "        de las imagenes de entrada y de los parámetros introducidos\"\"\"\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(w, 8, 4), 4, 2)\n",
    "        convh = conv2d_size_out(conv2d_size_out(h, 8, 4), 4, 2)\n",
    "        self.linear_input_size = convw * convh * 32\n",
    "\n",
    "        self.hl = nn.Linear(self.linear_input_size, 256)\n",
    "        self.ol = nn.Linear(256, outputs)\n",
    "\n",
    "    \"\"\"Devuelve un vector con el valor de las acciones posibles\"\"\"\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(1, self.linear_input_size)\n",
    "\n",
    "        return self.ol(self.hl(x))\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redes inicializadas\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "#-------------------- Inicialización de redes ----------------------------\n",
    "##########################################################################\n",
    "screen_height, screen_width = 84, 84\n",
    "\n",
    "red_politica = DQN(screen_height, screen_width, numAcciones).to(device)\n",
    "red_objetivo = DQN(screen_height, screen_width, numAcciones).to(device)\n",
    "red_objetivo.load_state_dict(red_politica.state_dict())\n",
    "red_objetivo.eval()\n",
    "scoreList = []\n",
    "optimizer = optim.RMSprop(red_politica.parameters())\n",
    "print(\"Redes inicializadas\")\n",
    "\n",
    "# #----------------------- Carga red de fichero ------------------#\n",
    "# pickle_in = open('listaScore','rb')                       #\n",
    "# scoreList = p.load(pickle_in)                                   #\n",
    "# pickle_in.close()                                               #\n",
    "#                                                                 #\n",
    "\n",
    "# # pickle_in = open('Memory','rb')                           #\n",
    "# # memory.memory = p.load(pickle_in)                               #\n",
    "# # pickle_in.close()                                               #\n",
    "#                                                                 #\n",
    "#                                                                 #\n",
    "# red_politica.load_state_dict(torch.load('RedPolitica.pt'))#\n",
    "# red_objetivo.load_state_dict(torch.load('RedObjetivo.pt'))#\n",
    "# #---------------------------------------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#-------------------------------------- Selector de acciones -------------------------------\n",
    "############################################################################################\n",
    "def action_selection(state):\n",
    "    global epsilon\n",
    "    \n",
    "    epsilon = epsilon-EPS_DECAY\n",
    "    if epsilon < EPS_MIN:\n",
    "        epsilon = EPS_MIN\n",
    "    \n",
    "\n",
    "    if random.randint(0, 100)/100 < epsilon:\n",
    "        return random.randrange(numAcciones)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return  red_politica(state).max(1)[1]\n",
    "\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comienzo del entrenamiento:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergio\\AppData\\Local\\Temp\\ipykernel_11788\\1856560556.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  estado = torch.Tensor(ImageBuffer)\n",
      "C:\\Users\\Sergio\\AppData\\Local\\Temp\\ipykernel_11788\\2978827812.py:13: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  estado = np.array(estado)\n",
      "C:\\Users\\Sergio\\AppData\\Local\\Temp\\ipykernel_11788\\2978827812.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  estado = np.array(estado)\n",
      "C:\\Users\\Sergio\\AppData\\Local\\Temp\\ipykernel_11788\\2978827812.py:14: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  estado_sig = np.array(estado_sig)\n",
      "C:\\Users\\Sergio\\AppData\\Local\\Temp\\ipykernel_11788\\2978827812.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  estado_sig = np.array(estado_sig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partida 0 acabada, recompensa acumulada 3.0\n",
      "Partida 100 acabada, recompensa acumulada 2.0\n",
      "Partida 200 acabada, recompensa acumulada 3.0\n",
      "Entranamiento de 1M de partidas acabado\n"
     ]
    }
   ],
   "source": [
    "############################################################################################\n",
    "#-------------------------------------- Bucle de entrenamiento -------------------------------\n",
    "############################################################################################\n",
    "print(\"Comienzo del entrenamiento:\")\n",
    "\n",
    "episodios = 300\n",
    "medidaTotal = []\n",
    "for partida in range(episodios):\n",
    "    env.reset()\n",
    "    ImageBuffer = []\n",
    "    for frame in range(K):\n",
    "        ImageBuffer.append(process_image())\n",
    "        \n",
    "    estado = torch.Tensor(ImageBuffer)\n",
    "\n",
    "    score = 0\n",
    "    medidaPartida = []\n",
    "    for j in count():\n",
    "        accion = action_selection(estado) \n",
    "\n",
    "        recompensa=0\n",
    "        sigEstado = []\n",
    "        for frame in range(K):\n",
    "            sigImg, r, done, _ = env.step(accion)\n",
    "            sigEstado.append(process_image(sigImg))\n",
    "            recompensa+=r\n",
    "        \n",
    "        if not done:\n",
    "            sigEstado = torch.Tensor(sigEstado)\n",
    "        else:\n",
    "            sigEstado = None\n",
    "            \n",
    "        memory.save(estado, accion, sigEstado, recompensa)\n",
    "\n",
    "        estado = sigEstado\n",
    "        score+=recompensa\n",
    "        if j%100 == 0 and j != 0 :\n",
    "            medidaPartida.append(optimize_model())\n",
    "            red_objetivo.load_state_dict(red_politica.state_dict())\n",
    "\n",
    "        if done:\n",
    "            medidaPartida.append(optimize_model())\n",
    "            scoreList.append(score)\n",
    "            break\n",
    "    \n",
    "    medidaTotal.append(np.mean(medidaPartida))\n",
    "    grafico = plt.plot(medidaTotal)\n",
    "    plt.savefig(\"plotMedida.jpg\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    if partida % 100 == 0:\n",
    "        logger.debug(\"Partida {} acabada, recompensa acumulada {}\".format(partida, score))\n",
    "        torch.save(red_objetivo.state_dict(), \"RedObjetivo.pt\")\n",
    "        torch.save(red_politica.state_dict(), \"RedPolitica.pt\")\n",
    "\n",
    "        outputFile = open('listaScore', 'wb')\n",
    "        p.dump(scoreList, outputFile)\n",
    "        outputFile.close()\n",
    "\n",
    "logger.debug(\"Entranamiento de 1M de partidas acabado\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e1b8a8ebc11f2236ca6e01d7f89bd219fcb3078f99dde0c4b36d6075860c1c3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

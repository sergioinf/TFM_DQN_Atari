{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Sergio\\TFM\\TFM_DQN_Atari\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pickle as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with: cuda\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Breakout-v5', render_mode=\"human\")\n",
    "\n",
    "numAcciones = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training with:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "#------------------- Get image from enviromet ------------------------\n",
    "######################################################################\n",
    "\n",
    "def get_image():\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                    #T.Resize(95, interpolation=Image.CUBIC),\n",
    "                    T.Grayscale(num_output_channels=1),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "    screen = resize(screen).unsqueeze(0)\n",
    "    return screen\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#--------------------------------------- Memory ---------------------------------------\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "#Renovar memoria\n",
    "    def save(self, state, action, next_state, reward):\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices     = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        \n",
    "        states      = []\n",
    "        actions     = []\n",
    "        next_states = []\n",
    "        rewards     = []\n",
    "\n",
    "\n",
    "        for idx in indices: \n",
    "            states.append(self.memory[idx][0])\n",
    "            actions.append(self.memory[idx][1])\n",
    "            next_states.append(self.memory[idx][2])\n",
    "            rewards.append(self.memory[idx][3])\n",
    "        \n",
    "        return states, actions, rewards, next_states\n",
    "        #return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(10000)\n",
    "BATCH_SIZE = 32\n",
    "EPS_START = 0.9\n",
    "EPS_DECAY = 0.045\n",
    "EPS_MIN = 0.02\n",
    "epsilon = EPS_START\n",
    "gamma = 0.99\n",
    "it = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#------------------------------- Memory Optimizer -------------------------------------\n",
    "#######################################################################################\n",
    "\n",
    "def optimize_model():\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    estado, accion, recompensa, estado_sig = batch\n",
    "\n",
    "    estado = np.array(estado)\n",
    "    estado_sig = np.array(estado_sig)\n",
    "\n",
    "    try:\n",
    "        listaNones = np.where(estado_sig == None)\n",
    "    except:\n",
    "        listaNones = []\n",
    "\n",
    "    tensor_accion       = torch.Tensor(accion).to(device)\n",
    "    tensor_recompensa   = torch.Tensor(recompensa).to(device)\n",
    "\n",
    "    estado_sig[listaNones] = estado[listaNones]\n",
    "\n",
    "    Qvalues = [red_politica(e).max(1)[0].item() for e in estado]\n",
    "    \n",
    "    QpValues = [red_objetivo(e).max(1)[0].item() for e in estado_sig]\n",
    "    QpValues = np.array(QpValues)\n",
    "    QpValues[listaNones] = 0.0\n",
    "\n",
    "\n",
    "    Qvalues = torch.Tensor(Qvalues).to(device)\n",
    "    Qvalues.requires_grad_()\n",
    "    QpValues = torch.Tensor(QpValues).to(device)\n",
    "    QpValues.requires_grad_()\n",
    "\n",
    "    valorEsperado = QpValues * gamma + tensor_recompensa\n",
    "\n",
    "    Qvalues.retain_grad()\n",
    "    valorEsperado.retain_grad()\n",
    "    \n",
    "    \n",
    "\n",
    "    loss = nn.MSELoss() \n",
    "    output = loss(valorEsperado, Qvalues)\n",
    "    optimizer.zero_grad()\n",
    "    output.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "#----------------------------------------- Estructura de la red ------------------------------------------------\n",
    "################################################################################################################\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        # Eliminar las batch\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2)\n",
    "        \n",
    "        \"\"\"Para calcular correctamente la salida, tenemos que linealizarla, esto depende de las dimensiones\n",
    "        de las imagenes de entrada y de los parámetros introducidos\"\"\"\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "\n",
    "        self.head = nn.Linear(linear_input_size, 256)\n",
    "        self.head = nn.Linear(256, outputs)\n",
    "\n",
    "    \"\"\"Devuelve un vector con el valor de las acciones posibles\"\"\"\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "\n",
    "#####################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redes inicializadas:\n",
      "Redes cargadas:\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "#-------------------- Inicialización de redes ----------------------------\n",
    "##########################################################################\n",
    "screen = get_image()\n",
    "_, _, screen_height, screen_width = screen.shape\n",
    "\n",
    "red_politica = DQN(screen_height, screen_width, numAcciones).to(device)\n",
    "red_objetivo = DQN(screen_height, screen_width, numAcciones).to(device)\n",
    "red_objetivo.load_state_dict(red_politica.state_dict())\n",
    "red_objetivo.eval()\n",
    "scoreList = []\n",
    "optimizer = optim.RMSprop(red_politica.parameters())\n",
    "print(\"Redes inicializadas:\")\n",
    "\n",
    "# #----------------------- Carga red de fichero ------------------#\n",
    "# pickle_in = open('listaScore','rb')                       #\n",
    "# scoreList = p.load(pickle_in)                                   #\n",
    "# pickle_in.close()                                               #\n",
    "#                                                                 #\n",
    "\n",
    "# # pickle_in = open('Memory','rb')                           #\n",
    "# # memory.memory = p.load(pickle_in)                               #\n",
    "# # pickle_in.close()                                               #\n",
    "#                                                                 #\n",
    "#                                                                 #\n",
    "# red_politica.load_state_dict(torch.load('RedPolitica.pt'))#\n",
    "# red_objetivo.load_state_dict(torch.load('RedObjetivo.pt'))#\n",
    "# #---------------------------------------------------------------#\n",
    "print(\"Redes cargadas:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#-------------------------------------- Selector de acciones -------------------------------\n",
    "############################################################################################\n",
    "def action_selection(state):\n",
    "    global epsilon, it\n",
    "    \n",
    "    epsilon = epsilon-EPS_DECAY/it\n",
    "    if epsilon < EPS_MIN:\n",
    "        epsilon = EPS_MIN\n",
    "    \n",
    "    it += 1\n",
    "\n",
    "    if random.randint(0, 100)/100 < epsilon:\n",
    "        return random.randrange(numAcciones)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return  red_politica(state).max(1)[1]\n",
    "\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comienzo del entrenamiento:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x15200 and 256x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Sergio\\TFM\\TFM_DQN_Atari\\DQN copy.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN%20copy.ipynb#ch0000009?line=15'>16</a>\u001b[0m score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN%20copy.ipynb#ch0000009?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m count():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN%20copy.ipynb#ch0000009?line=17'>18</a>\u001b[0m     accion \u001b[39m=\u001b[39m action_selection(estado)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN%20copy.ipynb#ch0000009?line=18'>19</a>\u001b[0m     estadoPrueba, recompensa, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(accion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN%20copy.ipynb#ch0000009?line=20'>21</a>\u001b[0m     score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m recompensa\n",
      "\u001b[1;32md:\\Sergio\\TFM\\TFM_DQN_Atari\\DQN copy.ipynb Cell 9'\u001b[0m in \u001b[0;36maction_selection\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN%20copy.ipynb#ch0000008?line=14'>15</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN%20copy.ipynb#ch0000008?line=15'>16</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN%20copy.ipynb#ch0000008?line=16'>17</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m  red_politica(state)\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32md:\\Sergio\\TFM\\TFM_DQN_Atari\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\Sergio\\TFM\\TFM_DQN_Atari\\DQN copy.ipynb Cell 7'\u001b[0m in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN%20copy.ipynb#ch0000006?line=31'>32</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN%20copy.ipynb#ch0000006?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x)))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Sergio/TFM/TFM_DQN_Atari/DQN%20copy.ipynb#ch0000006?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead(x\u001b[39m.\u001b[39;49mview(x\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m), \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[1;32md:\\Sergio\\TFM\\TFM_DQN_Atari\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Sergio\\TFM\\TFM_DQN_Atari\\env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///d%3A/Sergio/TFM/TFM_DQN_Atari/env/lib/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x15200 and 256x4)"
     ]
    }
   ],
   "source": [
    "############################################################################################\n",
    "#-------------------------------------- Bucle de entrenamiento -------------------------------\n",
    "############################################################################################\n",
    "print(\"Comienzo del entrenamiento:\")\n",
    "\n",
    "episodios = 10000000\n",
    "for i in range(episodios):\n",
    "    env.reset()\n",
    "    screen_1 = get_image()\n",
    "    screen_2 = get_image()\n",
    "\n",
    "    estado = screen_2-screen_1\n",
    "\n",
    "    # Cambiar la diferencia de las pantallas por un cubo con 4 pantallas juntas\n",
    "\n",
    "    score = 0\n",
    "    for j in count():\n",
    "        accion = action_selection(estado)\n",
    "        estadoPrueba, recompensa, done, _ = env.step(accion)\n",
    "        \n",
    "        score += recompensa\n",
    "        screen1 = screen_2\n",
    "        screen_2 = get_image()\n",
    "        \n",
    "        if not done:\n",
    "            sig_estado = screen_2-screen_1\n",
    "        else:\n",
    "            sig_estado = None\n",
    "            recompensa = -1000\n",
    "            \n",
    "        memory.save(estado, accion, sig_estado, recompensa)\n",
    "\n",
    "        estado = sig_estado\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            scoreList.append(score)\n",
    "            break\n",
    "    print(\"Partida {} acabada, recompensa acumulada {}\".format(i, score))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        red_objetivo.load_state_dict(red_politica.state_dict())\n",
    "        torch.save(red_objetivo.state_dict(), \"RedObjetivo.pt\")\n",
    "        torch.save(red_politica.state_dict(), \"RedPolitica.pt\")\n",
    "\n",
    "        outputFile = open('Memory', 'wb')\n",
    "        p.dump(memory.memory, outputFile)\n",
    "        outputFile.close()\n",
    "\n",
    "        outputFile = open('listaScore', 'wb')\n",
    "        p.dump(scoreList, outputFile)\n",
    "        outputFile.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a381338f764c2e2cde69e871487c18b603a6ed2174a69a63573e476f4f348b8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
